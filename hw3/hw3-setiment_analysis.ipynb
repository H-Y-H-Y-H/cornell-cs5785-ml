{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import sklearn.metrics as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "a = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "sys.stdout = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive label ratio is 0.500000\n"
     ]
    }
   ],
   "source": [
    "dt = np.dtype([('text','object'), ('label', 'int16')])\n",
    "\n",
    "amazon = np.loadtxt(\"data/amazon_cells_labelled.txt\", dtype=dt, delimiter='\\t', comments=\"THISISCOMMENT\")\n",
    "yelp = np.loadtxt(\"data/yelp_labelled.txt\", dtype=dt, delimiter='\\t', comments=\"THISISCOMMENT\")\n",
    "imdb = np.loadtxt(\"data/imdb_labelled.txt\", dtype=dt, delimiter='\\t', comments=\"THISISCOMMENT\")\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "test_ratio = 0.2\n",
    "\n",
    "for d in [amazon, yelp, imdb]:\n",
    "    tr, te = train_test_split(d, test_size=test_ratio, stratify=d['label'])\n",
    "    train_list.append(tr)\n",
    "    test_list.append(te)\n",
    "\n",
    "train = np.row_stack((x.reshape(-1, 1) for x in train_list))\n",
    "test = np.row_stack((x.reshape(-1, 1) for x in test_list))\n",
    "\n",
    "pos_ratio = (train[train['label'] == 1].shape[0] + test[test['label'] == 1].shape[0]) / float(train.shape[0] + test.shape[0])\n",
    "print \"Positive label ratio is %f\" % pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "class BagOfWordsVectorizer(object):\n",
    "    def __init__(self):\n",
    "        self.func_lemmatize = WordNetLemmatizer().lemmatize\n",
    "\n",
    "        self.STRIP_CHARS = string.punctuation + ' '\n",
    "        self.PUNCTUATIONS = set(string.punctuation)\n",
    "\n",
    "#         self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        self.STOP_WORDS = set([])\n",
    "\n",
    "\n",
    "        self.APOSTROPHE_DICT = {\"don't\": \"donot\", \"doesn't\": \"donot\", \"didn't\": \"donot\", \n",
    "                                \"isn't\": \"isnot\", \"aren't\": \"isnot\", \n",
    "                                \"i'm\": \"i\", \"you're\": \"you\", \"they're\": \"they\", \"he's\": \"he\", \"she's\": \"she\", \n",
    "                                \"it's\": \"it\", \"i've\": \"i\", \"you've\": \"you\",\n",
    "                                \"ha\": \"has\"}\n",
    "        self._vocabulary = None\n",
    "        self._lookup_voc = None\n",
    "        \n",
    "    def _strip_and_lemmatize(self, word, chars):\n",
    "#         word = str(self.func_lemmatize(word.strip(chars)))\n",
    "        word = str(self.func_lemmatize(word))\n",
    "        \n",
    "        if word in self.APOSTROPHE_DICT:\n",
    "            word = self.APOSTROPHE_DICT[word]\n",
    "            \n",
    "#         return word\n",
    "        return word.translate(None, self.STRIP_CHARS)\n",
    "    \n",
    "    def _tokenize_and_clean(self, sentence):\n",
    "#         _ = [self._strip_and_lemmatize(s, self.STRIP_CHARS) \n",
    "#              for s in sentence.lower().split(' ') if (s not in self.STOP_WORDS and len(s) > 0)]\n",
    "        _ = [self._strip_and_lemmatize(s, self.STRIP_CHARS) \n",
    "             for s in re.split(r'[^a-z\\']', sentence.lower()) if (s not in self.STOP_WORDS and len(s) > 0)]\n",
    "    \n",
    "        return filter(lambda s: len(s) > 0 and \n",
    "                               s not in self.STOP_WORDS and \n",
    "                               s not in self.PUNCTUATIONS, _)\n",
    "    def tokenize(self, X):\n",
    "        return map(self._tokenize_and_clean, X)\n",
    "    \n",
    "    \n",
    "    def _l1(self, x):\n",
    "        _l = len(x.nonzero()[0])\n",
    "        if _l > 0:\n",
    "            return x / float(_l)\n",
    "        else:\n",
    "            return np.zeros((len(x),))\n",
    "        \n",
    "    def _l2(self, x):\n",
    "        _l = len(x.nonzero()[0])\n",
    "        if _l > 0:\n",
    "            return x / float(np.sqrt(np.dot(x, x.T)))\n",
    "        else:\n",
    "            return np.zeros((len(x),))\n",
    "\n",
    "    def l1_normalize(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._l1(x), 1, X)\n",
    "\n",
    "    def l2_normalize(self, X):\n",
    "        return np.apply_along_axis(lambda x: self._l2(x), 1, X)\n",
    "    \n",
    "    \n",
    "    def _bag_of_words(self, words):\n",
    "        bag = np.zeros((self._voc_len,), dtype=np.int)\n",
    "        for word in words:\n",
    "            try:\n",
    "                bag[self._vocabulary[word]] += 1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return bag\n",
    "    \n",
    "        \n",
    "    def fit(self, X):\n",
    "        self.fit_transform(X)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, normalize=None):\n",
    "        # 1. build vocabulary\n",
    "        tokens = self.tokenize(X)\n",
    "        \n",
    "        _ = set(np.concatenate(tokens))\n",
    "        \n",
    "        self._lookup_voc = {}\n",
    "        self._vocabulary = {}\n",
    "        for i, x in enumerate(_):\n",
    "            self._lookup_voc[i] = x\n",
    "            self._vocabulary[x] = i\n",
    "        \n",
    "        self._voc_len = len(_)\n",
    "        \n",
    "        # 2. generate X\n",
    "        _ = np.array(map(self._bag_of_words, tokens))\n",
    "        if normalize is None:\n",
    "            return _\n",
    "        elif normalize == 'l2':\n",
    "            return self.l2_normalize(_)\n",
    "        else:\n",
    "            return self.l1_normalize(_)\n",
    "            \n",
    "    def transform(self, X, normalize=None):\n",
    "        tokens = self.tokenize(X)\n",
    "#         print tokens\n",
    "        if self._vocabulary is not None:\n",
    "            _ = np.array(map(self._bag_of_words, tokens)) \n",
    "            \n",
    "            if normalize is None:\n",
    "                return _\n",
    "            elif normalize == 'l2':\n",
    "                return self.l2_normalize(_)\n",
    "            else:\n",
    "                return self.l1_normalize(_)\n",
    "        else:\n",
    "            return None\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class KMeans():\n",
    "    def __init__(self, max_iter=1000):\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, k):\n",
    "        self.X = X\n",
    "        self.N, self.p = X.shape\n",
    "        self.k = k\n",
    "        \n",
    "        _ = set([])\n",
    "        while len(_) < k:\n",
    "            _.add(random.randint(0, self.N - 1))\n",
    "        self.center = np.copy(X[tuple(_), :])\n",
    "        assert self.center.shape == (self.k, self.p)\n",
    "        \n",
    "        self.status = np.zeros((self.N,))\n",
    "        \n",
    "        converge = False\n",
    "        iter_count = 0\n",
    "        while not converge and iter_count < self.max_iter:\n",
    "            converge = True\n",
    "            _status = np.apply_along_axis(self._get_center_idx, 1, X)\n",
    "            \n",
    "            if not np.array_equal(self.status, _status):\n",
    "                converge = False\n",
    "                self._update_center(_status)\n",
    "                self.status = _status\n",
    "            \n",
    "            iter_count += 1\n",
    "        self.iter_count = iter_count\n",
    "        \n",
    "    def _get_center_idx(self, x):\n",
    "        return np.argmin(np.sum((self.center - x) ** 2, axis=1), axis=0)\n",
    "        \n",
    "    def _update_center(self, status):\n",
    "        for i in range(self.k):\n",
    "            _center = np.mean(self.X[status == i], axis=0)\n",
    "            self.center[i] = _center\n",
    "    \n",
    "    def get_center(self):\n",
    "        return self.center # shape: k*p\n",
    "    \n",
    "    def get_status(self):\n",
    "        return self.status\n",
    "    \n",
    "    def get_cluster(self, k):\n",
    "        return self.X[self.status == k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1200\n",
      "1199\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans()\n",
    "bowv = NGramVectorizer(2)\n",
    "train_bag = bowv.fit_transform(train['text'][:, 0], normalize=None)\n",
    "kmeans.fit(train_bag, 2)\n",
    "train_center = kmeans.get_center()\n",
    "train_status = kmeans.get_status()\n",
    "cluster0 = train['label'][train_status == 0, 0]\n",
    "cluster1 = train['label'][train_status == 1, 0]\n",
    "print np.sum(cluster0)\n",
    "print cluster0.shape[0] - np.sum(cluster0)\n",
    "print np.sum(cluster1)\n",
    "print cluster1.shape[0] - np.sum(cluster1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster0 = [bowv._lookup_voc[_idx] for _idx in train_center[0].nonzero()[0]]\n",
    "cluster1 = [bowv._lookup_voc[_idx] for _idx in train_center[1].nonzero()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['limited',\n",
       " 'todays',\n",
       " 'secondly',\n",
       " 'yellow',\n",
       " 'sleek',\n",
       " 'four',\n",
       " 'sleep',\n",
       " 'appetite',\n",
       " 'poorly',\n",
       " 'whose',\n",
       " 'replacementr',\n",
       " 'buddy',\n",
       " 'spew',\n",
       " 'rpg',\n",
       " 'sweetest',\n",
       " 'charles',\n",
       " 'insipid',\n",
       " 'whatsoever',\n",
       " 'under',\n",
       " 'rickman',\n",
       " 'lord',\n",
       " 'welcome',\n",
       " 'worth',\n",
       " 'sinking',\n",
       " 'struck',\n",
       " 'rescue',\n",
       " 'shoot',\n",
       " 'smack',\n",
       " 'every',\n",
       " 'jack',\n",
       " 'bertolucci',\n",
       " 'trailer',\n",
       " 'believed',\n",
       " 'school',\n",
       " 'wooden',\n",
       " 'loneliness',\n",
       " 'coppola',\n",
       " 'upload',\n",
       " 'girolamo',\n",
       " 'frozen',\n",
       " 'haggis',\n",
       " 'falwell',\n",
       " 'ups',\n",
       " 'enjoy',\n",
       " 'force',\n",
       " 'specially',\n",
       " 'phones',\n",
       " 'consistent',\n",
       " 'bacon',\n",
       " 'budget',\n",
       " 'chef',\n",
       " 'surrounding',\n",
       " 'second',\n",
       " 'louder',\n",
       " 'mesmerising',\n",
       " 'gorman',\n",
       " 'oscar',\n",
       " 'monster',\n",
       " 'even',\n",
       " 'established',\n",
       " 'everyones',\n",
       " 'microsofts',\n",
       " 'ruthless',\n",
       " 'cooking',\n",
       " 'asia',\n",
       " 'achille',\n",
       " 'veggitarian',\n",
       " 'above',\n",
       " 'new',\n",
       " 'net',\n",
       " 'ever',\n",
       " 'told',\n",
       " 'simmering',\n",
       " 'hero',\n",
       " 'philippa',\n",
       " 'reporter',\n",
       " 'join',\n",
       " 'never',\n",
       " 'disposable',\n",
       " 'meh',\n",
       " 'here',\n",
       " 'screamy',\n",
       " 'protection',\n",
       " 'greedy',\n",
       " 'cardboard',\n",
       " 'mozzarella',\n",
       " 'dee',\n",
       " 'celebration',\n",
       " 'dry',\n",
       " 'remotely',\n",
       " 'balance',\n",
       " 'daughter',\n",
       " 'uplifting',\n",
       " 'study',\n",
       " 'pi',\n",
       " 'controversy',\n",
       " 'kudos',\n",
       " 'smoke',\n",
       " 'military',\n",
       " 'theres',\n",
       " 'golden',\n",
       " 'fantastic',\n",
       " 'owed',\n",
       " 'straw',\n",
       " 'natural',\n",
       " 'reenactment',\n",
       " 'highly',\n",
       " 'moral',\n",
       " 'latifas',\n",
       " 'boast',\n",
       " 'glance',\n",
       " 'total',\n",
       " 'schilling',\n",
       " 'unit',\n",
       " 'remaining',\n",
       " 'plot',\n",
       " 'would',\n",
       " 'negative',\n",
       " 'insult',\n",
       " 'call',\n",
       " 'giallo',\n",
       " 'recommend',\n",
       " 'strike',\n",
       " 'type',\n",
       " 'hollywood',\n",
       " 'sushi',\n",
       " 'tranquillity',\n",
       " 'brings',\n",
       " 'wart',\n",
       " 'award',\n",
       " 'hurt',\n",
       " 'glass',\n",
       " 'reversible',\n",
       " 'excellent',\n",
       " 'hole',\n",
       " 'hold',\n",
       " 'must',\n",
       " 'me',\n",
       " 'directorial',\n",
       " 'locked',\n",
       " 'room',\n",
       " 'shame',\n",
       " 'setup',\n",
       " 'work',\n",
       " 'littered',\n",
       " 'worn',\n",
       " 'native',\n",
       " 'mp',\n",
       " 'install',\n",
       " 'already',\n",
       " 'elegantly',\n",
       " 'my',\n",
       " 'example',\n",
       " 'vodka',\n",
       " 'estate',\n",
       " 'give',\n",
       " 'indie',\n",
       " 'climax',\n",
       " 'beautifully',\n",
       " 'want',\n",
       " 'absolute',\n",
       " 'preferably',\n",
       " 'biographical',\n",
       " 'end',\n",
       " 'farce',\n",
       " 'keen',\n",
       " 'provide',\n",
       " 'describes',\n",
       " 'returning',\n",
       " 'feature',\n",
       " 'theyd',\n",
       " 'how',\n",
       " 'amazing',\n",
       " 'env',\n",
       " 'answer',\n",
       " 'disappoint',\n",
       " 'charging',\n",
       " 'pizza',\n",
       " 'badly',\n",
       " 'description',\n",
       " 'beauty',\n",
       " 'funny',\n",
       " 'mess',\n",
       " 'after',\n",
       " 'imaginable',\n",
       " 'underacting',\n",
       " 'wrong',\n",
       " 'summarize',\n",
       " 'law',\n",
       " 'purchase',\n",
       " 'superbad',\n",
       " 'austere',\n",
       " 'third',\n",
       " 'move',\n",
       " 'subplots',\n",
       " 'childhood',\n",
       " 'squib',\n",
       " 'greek',\n",
       " 'green',\n",
       " 'fan',\n",
       " 'operate',\n",
       " 'order',\n",
       " 'wind',\n",
       " 'wine',\n",
       " 'emotion',\n",
       " 'executed',\n",
       " 'interpretation',\n",
       " 'firehouse',\n",
       " 'drivng',\n",
       " 'sprout',\n",
       " 'over',\n",
       " 'satisfied',\n",
       " 'jerry',\n",
       " 'london',\n",
       " 'oven',\n",
       " 'keyboard',\n",
       " 'crumby',\n",
       " 'before',\n",
       " 'fis',\n",
       " 'difference',\n",
       " 'fit',\n",
       " 'hypocrisy',\n",
       " 'somewhere',\n",
       " 'fix',\n",
       " 'writing',\n",
       " 'better',\n",
       " 'thomerson',\n",
       " 'production',\n",
       " 'compelling',\n",
       " 'condition',\n",
       " 'overnite',\n",
       " 'easier',\n",
       " 'versus',\n",
       " 'then',\n",
       " 'them',\n",
       " 'combination',\n",
       " 'unemployed',\n",
       " 'break',\n",
       " 'plater',\n",
       " 'they',\n",
       " 'yourself',\n",
       " 'humanity',\n",
       " 'bank',\n",
       " 'bread',\n",
       " 'represents',\n",
       " 'indictment',\n",
       " 'meat',\n",
       " 'trinity',\n",
       " 'regularly',\n",
       " 'reasonably',\n",
       " 'sanyo',\n",
       " 'wilkinsons',\n",
       " 'ventilation',\n",
       " 'bellucci',\n",
       " 'reasonable',\n",
       " 'each',\n",
       " 'volcano',\n",
       " 'went',\n",
       " 'side',\n",
       " 'luck',\n",
       " 'pretext',\n",
       " 'series',\n",
       " 'masterful',\n",
       " 'group',\n",
       " 'arrives',\n",
       " 'message',\n",
       " 'hanky',\n",
       " 'velvet',\n",
       " 'network',\n",
       " 'driving',\n",
       " 'borrowed',\n",
       " 'washed',\n",
       " 'forty',\n",
       " 'cheesy',\n",
       " 'content',\n",
       " 'remorse',\n",
       " 'potted',\n",
       " 'reader',\n",
       " 'got',\n",
       " 'forth',\n",
       " 'asleep',\n",
       " 'grease',\n",
       " 'linear',\n",
       " 'cutest',\n",
       " 'written',\n",
       " 'little',\n",
       " 'free',\n",
       " 'standard',\n",
       " 'masterpiece',\n",
       " 'overacting',\n",
       " 'estevez',\n",
       " 'wanted',\n",
       " 'storm',\n",
       " 'ate',\n",
       " 'bates',\n",
       " 'created',\n",
       " 'inexpensive',\n",
       " 'genre',\n",
       " 'brooding',\n",
       " 'flawless',\n",
       " 'shipped',\n",
       " 'speedy',\n",
       " 'hopeless',\n",
       " 'moving',\n",
       " 'loud',\n",
       " 'stewart',\n",
       " 'industrial',\n",
       " 'render',\n",
       " 'mansonites',\n",
       " 'grade',\n",
       " 'accent',\n",
       " 'puck',\n",
       " 'veggie',\n",
       " 'rank',\n",
       " 'hook',\n",
       " 'downside',\n",
       " 'another',\n",
       " 'perabo',\n",
       " 'miserable',\n",
       " 'thick',\n",
       " 'researched',\n",
       " 'cheerless',\n",
       " 'rated',\n",
       " 'service',\n",
       " 'forgery',\n",
       " 'top',\n",
       " 'tot',\n",
       " 'thorsen',\n",
       " 'needed',\n",
       " 'compromise',\n",
       " 'too',\n",
       " 'hummh',\n",
       " 'john',\n",
       " 'unfunny',\n",
       " 'predictably',\n",
       " 'tool',\n",
       " 'convention',\n",
       " 'serve',\n",
       " 'took',\n",
       " 'waster',\n",
       " 'tardis',\n",
       " 'predictable',\n",
       " 'stoic',\n",
       " 'somewhat',\n",
       " 'helen',\n",
       " 'wasted',\n",
       " 'linksys',\n",
       " 'gake',\n",
       " 'stocking',\n",
       " 'spoiler',\n",
       " 'trek',\n",
       " 'wilkinson',\n",
       " 'treo',\n",
       " 'elegant',\n",
       " 'rate',\n",
       " 'seated',\n",
       " 'final',\n",
       " 'shower',\n",
       " 'hollander',\n",
       " 'fabulous',\n",
       " 'guilt',\n",
       " 'speaking',\n",
       " 'deserving',\n",
       " 'spoiled',\n",
       " 'k',\n",
       " 'joy',\n",
       " 'feeling',\n",
       " 'brilliant',\n",
       " 'screenplay',\n",
       " 'mini',\n",
       " 'lieutenant',\n",
       " 'respecting',\n",
       " 'navigate',\n",
       " 'modern',\n",
       " 'mind',\n",
       " 'youtube',\n",
       " 'talking',\n",
       " 'shell',\n",
       " 'seen',\n",
       " 'seem',\n",
       " 'hellish',\n",
       " 'tale',\n",
       " 'vandiver',\n",
       " 'giovanni',\n",
       " 'dozen',\n",
       " 'forced',\n",
       " 'abroad',\n",
       " 'strength',\n",
       " 'genuine',\n",
       " 'achievement',\n",
       " 'convenient',\n",
       " 'latter',\n",
       " 'oriented',\n",
       " 'gabriels',\n",
       " 'recommended',\n",
       " 'academy',\n",
       " 'transmit',\n",
       " 'yucky',\n",
       " 'explanation',\n",
       " 'crocs',\n",
       " 'blue',\n",
       " 'plantronincs',\n",
       " 'lion',\n",
       " 'though',\n",
       " 'thrilled',\n",
       " 'replenished',\n",
       " 'drifting',\n",
       " 'cinematic',\n",
       " 'dreary',\n",
       " 'flashback',\n",
       " 'regular',\n",
       " 'mouth',\n",
       " 'plenty',\n",
       " 'terror',\n",
       " 'mesquite',\n",
       " 'phantasm',\n",
       " 'jean',\n",
       " 'stylized',\n",
       " 'accessoryone',\n",
       " 'episode',\n",
       " 'gaudi',\n",
       " 'alarm',\n",
       " 'flop',\n",
       " 'doe',\n",
       " 'swamp',\n",
       " 'definitely',\n",
       " 'tech',\n",
       " 'entertaining',\n",
       " 'consumer',\n",
       " 'scream',\n",
       " 'came',\n",
       " 'overpriced',\n",
       " 'ringtones',\n",
       " 'random',\n",
       " 'brilliance',\n",
       " 'cancan',\n",
       " 'ending',\n",
       " 'austens',\n",
       " 'subtle',\n",
       " 'cheaply',\n",
       " 'lense',\n",
       " 'enjoyed',\n",
       " 'toast',\n",
       " 'finally',\n",
       " 'painfully',\n",
       " 'menu',\n",
       " 'explain',\n",
       " 'universal',\n",
       " 'sugar',\n",
       " 'theme',\n",
       " 'bordered',\n",
       " 'rick',\n",
       " 'disgust',\n",
       " 'chipotle',\n",
       " 'rice',\n",
       " 'regrettable',\n",
       " 'sporting',\n",
       " 'personality',\n",
       " 'wearing',\n",
       " 'do',\n",
       " 'wide',\n",
       " 'air',\n",
       " 'de',\n",
       " 'stop',\n",
       " 'appetizer',\n",
       " 'pocket',\n",
       " 'amazon',\n",
       " 'christopher',\n",
       " 'despite',\n",
       " 'nasty',\n",
       " 'dr',\n",
       " 'bat',\n",
       " 'bar',\n",
       " 'nicely',\n",
       " 'sour',\n",
       " 'hello',\n",
       " 'dipping',\n",
       " 'sandwich',\n",
       " 'bay',\n",
       " 'twice',\n",
       " 'bad',\n",
       " 'bec',\n",
       " 'grilled',\n",
       " 'steak',\n",
       " 'gotta',\n",
       " 'gallon',\n",
       " 'blew',\n",
       " 'miserably',\n",
       " 'keypad',\n",
       " 'disapoinment',\n",
       " 'roasted',\n",
       " 'depends',\n",
       " 'result',\n",
       " 'horrendous',\n",
       " 'fail',\n",
       " 'disturbing',\n",
       " 'armageddon',\n",
       " 'best',\n",
       " 'voyage',\n",
       " 'said',\n",
       " 'capacity',\n",
       " 'inappropriate',\n",
       " 'deadpan',\n",
       " 'away',\n",
       " 'continually',\n",
       " 'grandmother',\n",
       " 'finger',\n",
       " 'timeframe',\n",
       " 'discovery',\n",
       " 'we',\n",
       " 'omg',\n",
       " 'wa',\n",
       " 'rolled',\n",
       " 'smelled',\n",
       " 'transcendant',\n",
       " 'packaged',\n",
       " 'however',\n",
       " 'dustpan',\n",
       " 'dedication',\n",
       " 'bose',\n",
       " 'jonah',\n",
       " 'improve',\n",
       " 'received',\n",
       " 'essentially',\n",
       " 'cow',\n",
       " 'brat',\n",
       " 'resistant',\n",
       " 'sculpture',\n",
       " 'ill',\n",
       " 'subverting',\n",
       " 'against',\n",
       " 'loewenhielms',\n",
       " 'hummus',\n",
       " 'receives',\n",
       " 'studio',\n",
       " 'asked',\n",
       " 'con',\n",
       " 'tone',\n",
       " 'character',\n",
       " 'vain',\n",
       " 'sabotage',\n",
       " 'cult',\n",
       " 'massive',\n",
       " 'quinn',\n",
       " 'humming',\n",
       " 'trust',\n",
       " 'san',\n",
       " 'bathroom',\n",
       " 'clip',\n",
       " 'beef',\n",
       " 'source',\n",
       " 'binge',\n",
       " 'three',\n",
       " 'been',\n",
       " 'quickly',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'much',\n",
       " 'interest',\n",
       " 'basic',\n",
       " 'expected',\n",
       " 'fry',\n",
       " 'lovely',\n",
       " 'correction',\n",
       " 'website',\n",
       " 'juano',\n",
       " 'life',\n",
       " 'sydney',\n",
       " 'mushroom',\n",
       " 'buyit',\n",
       " 'worker',\n",
       " 'comprehensible',\n",
       " 'coherent',\n",
       " 'child',\n",
       " 'worked',\n",
       " 'chill',\n",
       " 'has',\n",
       " 'tank',\n",
       " 'peculiarity',\n",
       " 'resume',\n",
       " 'n',\n",
       " 'lesser',\n",
       " 'ugly',\n",
       " 'near',\n",
       " 'melted',\n",
       " 'rochon',\n",
       " 'voice',\n",
       " 'armband',\n",
       " 'vegetarian',\n",
       " 'mistake',\n",
       " 'stink',\n",
       " 'honeslty',\n",
       " 'hunan',\n",
       " 'played',\n",
       " 'iq',\n",
       " 'is',\n",
       " 'ir',\n",
       " 'it',\n",
       " 'player',\n",
       " 'cant',\n",
       " 'im',\n",
       " 'brutal',\n",
       " 'exterior',\n",
       " 'in',\n",
       " 'violin',\n",
       " 'tremendously',\n",
       " 'mouse',\n",
       " 'id',\n",
       " 'sever',\n",
       " 'if',\n",
       " 'containing',\n",
       " 'credit',\n",
       " 'patent',\n",
       " 'suggest',\n",
       " 'make',\n",
       " 'wound',\n",
       " 'airport',\n",
       " 'clearly',\n",
       " 'nun',\n",
       " 'backed',\n",
       " 'drawback',\n",
       " 'potentially',\n",
       " 'belly',\n",
       " 'vegetable',\n",
       " 'success',\n",
       " 'several',\n",
       " 'toasted',\n",
       " 'european',\n",
       " 'fairly',\n",
       " 'shatner',\n",
       " 'boiled',\n",
       " 'welsh',\n",
       " 'hang',\n",
       " 'evil',\n",
       " 'hand',\n",
       " 'delight',\n",
       " 'garlic',\n",
       " 'characters',\n",
       " 'tuna',\n",
       " 'overt',\n",
       " 'kid',\n",
       " 'butter',\n",
       " 'dylan',\n",
       " 'kept',\n",
       " 'charlie',\n",
       " 'min',\n",
       " 'suffering',\n",
       " 'campy',\n",
       " 'contact',\n",
       " 'greatest',\n",
       " 'mother',\n",
       " 'the',\n",
       " 'veteran',\n",
       " 'left',\n",
       " 'impression',\n",
       " 'just',\n",
       " 'gods',\n",
       " 'laptop',\n",
       " 'absolutley',\n",
       " 'completed',\n",
       " 'identify',\n",
       " 'thanks',\n",
       " 'human',\n",
       " 'thug',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'previous',\n",
       " 'terrific',\n",
       " 'unique',\n",
       " 'dining',\n",
       " 'had',\n",
       " 'hay',\n",
       " 'transformed',\n",
       " 'teenager',\n",
       " 'easy',\n",
       " 'zillion',\n",
       " 'pitiful',\n",
       " 'save',\n",
       " 'tolerable',\n",
       " 'deadly',\n",
       " 'gave',\n",
       " 'casting',\n",
       " 'dignity',\n",
       " 'mayo',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'seperate',\n",
       " 'background',\n",
       " 'destroy',\n",
       " 'judge',\n",
       " 'replace',\n",
       " 'retreat',\n",
       " 'apart',\n",
       " 'articulated',\n",
       " 'defensemen',\n",
       " 'nude',\n",
       " 'manual',\n",
       " 'intermittently',\n",
       " 'informative',\n",
       " 'night',\n",
       " 'cheesiness',\n",
       " 'eew',\n",
       " 'towards',\n",
       " 'zombiez',\n",
       " 'cowardice',\n",
       " 'superlative',\n",
       " 'right',\n",
       " 'old',\n",
       " 'ole',\n",
       " 'deal',\n",
       " 'people',\n",
       " 'deaf',\n",
       " 'somehow',\n",
       " 'dead',\n",
       " 'jennifer',\n",
       " 'shortlist',\n",
       " 'bore',\n",
       " 'perfected',\n",
       " 'confusing',\n",
       " 'anita',\n",
       " 'shakespears',\n",
       " 'proceeding',\n",
       " 'tracked',\n",
       " 'for',\n",
       " 'bottom',\n",
       " 'phenomenal',\n",
       " 'laselva',\n",
       " 'artiness',\n",
       " 'ice',\n",
       " 'creative',\n",
       " 'violinist',\n",
       " 'everything',\n",
       " 'asking',\n",
       " 'landscape',\n",
       " 'ghibili',\n",
       " 'adorable',\n",
       " 'christmas',\n",
       " 'bisque',\n",
       " 'core',\n",
       " 'tartar',\n",
       " 'bold',\n",
       " 'corn',\n",
       " 'speak',\n",
       " 'discount',\n",
       " 'losing',\n",
       " 'memorable',\n",
       " 'post',\n",
       " 'super',\n",
       " 'reflected',\n",
       " 'relaxing',\n",
       " 'plug',\n",
       " 'limitation',\n",
       " 'catchy',\n",
       " 'dinner',\n",
       " 'misleading',\n",
       " 'plus',\n",
       " 'afternoon',\n",
       " 'emilio',\n",
       " 'lacked',\n",
       " 'curry',\n",
       " 'sympathetic',\n",
       " 'nerve',\n",
       " 'contains',\n",
       " 'presence',\n",
       " 'wasting',\n",
       " 'material',\n",
       " 'puzzle',\n",
       " 'chimp',\n",
       " 'son',\n",
       " 'down',\n",
       " 'rendering',\n",
       " 'adventure',\n",
       " 'coastal',\n",
       " 'narration',\n",
       " 'wrap',\n",
       " 'yawn',\n",
       " 'overhaul',\n",
       " 'crowd',\n",
       " 'greenstreet',\n",
       " 'support',\n",
       " 'flying',\n",
       " 'jealousy',\n",
       " 'vision',\n",
       " 'class',\n",
       " 'virgin',\n",
       " 'accordingly',\n",
       " 'way',\n",
       " 'greatness',\n",
       " 'music',\n",
       " 'war',\n",
       " 'happy',\n",
       " 'head',\n",
       " 'medium',\n",
       " 'form',\n",
       " 'offer',\n",
       " 'fascination',\n",
       " 'magnificent',\n",
       " 'oil',\n",
       " 'ford',\n",
       " 'wireless',\n",
       " 'heat',\n",
       " 'hear',\n",
       " 'diaper',\n",
       " 'true',\n",
       " 'especially',\n",
       " 'admiration',\n",
       " 'surprising',\n",
       " 'array',\n",
       " 'prevents',\n",
       " 'inside',\n",
       " 'bargain',\n",
       " 'tell',\n",
       " 'crystal',\n",
       " 'tucson',\n",
       " 'insincere',\n",
       " 'heimer',\n",
       " 'duet',\n",
       " 'latch',\n",
       " 'seafood',\n",
       " 'later',\n",
       " 'disgrace',\n",
       " 'classic',\n",
       " 'jabra',\n",
       " 'shined',\n",
       " 'proven',\n",
       " 'hungry',\n",
       " 'qwerty',\n",
       " 'promised',\n",
       " 'fashioned',\n",
       " 'believable',\n",
       " 'mediocre',\n",
       " 'annoying',\n",
       " 'check',\n",
       " 'constructed',\n",
       " 'rough',\n",
       " 'dying',\n",
       " 'no',\n",
       " 'marys',\n",
       " 'insane',\n",
       " 'relax',\n",
       " 'tip',\n",
       " 'actor',\n",
       " 'reality',\n",
       " 'mystifying',\n",
       " 'faux',\n",
       " 'interested',\n",
       " 'role',\n",
       " 'holding',\n",
       " 'escapism',\n",
       " 'smell',\n",
       " 'roll',\n",
       " 'realize',\n",
       " 'picture',\n",
       " 'theatrical',\n",
       " 'surprise',\n",
       " 'felt',\n",
       " 'rancheros',\n",
       " 'improvisation',\n",
       " 'fell',\n",
       " 'backdrop',\n",
       " 'spy',\n",
       " 'died',\n",
       " 'babysitting',\n",
       " 'jones',\n",
       " 'younger',\n",
       " 'apology',\n",
       " 'phone',\n",
       " 'moved',\n",
       " 'kristoffersen',\n",
       " 'together',\n",
       " 'warm',\n",
       " 'reception',\n",
       " 'time',\n",
       " 'push',\n",
       " 'serious',\n",
       " 'president',\n",
       " 'restored',\n",
       " 'stick',\n",
       " 'wanting',\n",
       " 'gratuity',\n",
       " 'managed',\n",
       " 'doctor',\n",
       " 'whoever',\n",
       " 'dance',\n",
       " 'skip',\n",
       " 'focus',\n",
       " 'integration',\n",
       " 'manager',\n",
       " 'marion',\n",
       " 'milk',\n",
       " 'recharge',\n",
       " 'witty',\n",
       " 'supposedly',\n",
       " 'bitpim',\n",
       " 'yummy',\n",
       " 'promote',\n",
       " 'flash',\n",
       " 'father',\n",
       " 'disgusting',\n",
       " 'vey',\n",
       " 'characterisation',\n",
       " 'charge',\n",
       " 'unpleasant',\n",
       " 'wed',\n",
       " 'discovering',\n",
       " 'suffered',\n",
       " 'southwest',\n",
       " 'division',\n",
       " 'smartphone',\n",
       " 'string',\n",
       " 'jerk',\n",
       " 'vegan',\n",
       " 'choice',\n",
       " 'bechard',\n",
       " 'hosting',\n",
       " 'cook',\n",
       " 'word',\n",
       " 'trouble',\n",
       " 'shenanigan',\n",
       " 'minute',\n",
       " 'cool',\n",
       " 'cardellini',\n",
       " 'impressive',\n",
       " 'helm',\n",
       " 'level',\n",
       " 'did',\n",
       " 'die',\n",
       " 'jamie',\n",
       " 'accidentally',\n",
       " 'cameo',\n",
       " 'leave',\n",
       " 'item',\n",
       " 'brownish',\n",
       " 'subway',\n",
       " 'burned',\n",
       " 'dit',\n",
       " 'guy',\n",
       " 'famed',\n",
       " 'upper',\n",
       " 'pork',\n",
       " 'regret',\n",
       " 'seasoning',\n",
       " 'dealing',\n",
       " 'sign',\n",
       " 'linda',\n",
       " 'cost',\n",
       " 'bombardment',\n",
       " 'dried',\n",
       " 'spacey',\n",
       " 'fort',\n",
       " 'fear',\n",
       " 'headset',\n",
       " 'heres',\n",
       " 'imagined',\n",
       " 'underbite',\n",
       " 'melt',\n",
       " 'current',\n",
       " 'nine',\n",
       " 'falling',\n",
       " 'boost',\n",
       " 'scripted',\n",
       " 'era',\n",
       " 'supporting',\n",
       " 'angelina',\n",
       " 'jury',\n",
       " 'morgan',\n",
       " 'learn',\n",
       " 'lange',\n",
       " 'french',\n",
       " 'understanding',\n",
       " 'water',\n",
       " 'corporation',\n",
       " 'upgrade',\n",
       " 'baseball',\n",
       " 'elses',\n",
       " 'alone',\n",
       " 'noir',\n",
       " 'along',\n",
       " 'appears',\n",
       " 'change',\n",
       " 'wait',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'disapppointment',\n",
       " 'incoming',\n",
       " 'thirty',\n",
       " 'mchattie',\n",
       " 'healthy',\n",
       " 'bos',\n",
       " 'proud',\n",
       " 'legit',\n",
       " 'suggestion',\n",
       " 'weird',\n",
       " 'pillow',\n",
       " 'emerge',\n",
       " 'dumb',\n",
       " 'love',\n",
       " 'consolation',\n",
       " 'humour',\n",
       " 'extra',\n",
       " ...]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 0.836666666667\n",
      "[[254  46]\n",
      " [ 52 248]]\n",
      "l1 0.75\n",
      "[[224  76]\n",
      " [ 74 226]]\n",
      "l2 0.788333333333\n",
      "[[235  65]\n",
      " [ 62 238]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for n in [None, 'l1', 'l2']:\n",
    "    bowv = BagOfWordsVectorizer()\n",
    "    train_bag = bowv.fit_transform(train['text'][:,0], normalize=n)\n",
    "    test_bag = bowv.transform(test['text'][:,0], normalize=n)\n",
    "\n",
    "    cls = LR()\n",
    "    cls.fit(train_bag, train['label'][:,0])\n",
    "    _pred = cls.predict(test_bag)\n",
    "    print n, len((_pred == test['label'][:,0]).nonzero()[0]) / float(test_bag.shape[0])\n",
    "    print me.confusion_matrix(test['label'][:,0], _pred)\n",
    "print '---'\n",
    "\n",
    "# for n in [None, 'l1', 'l2']:\n",
    "#     bowv = NGramVectorizer(2)\n",
    "#     train_bag = bowv.fit_transform(train['text'][:,0], normalize=n)\n",
    "#     test_bag = bowv.transform(test['text'][:,0], normalize=n)\n",
    "\n",
    "#     cls = LR()\n",
    "#     cls.fit(train_bag, train['label'][:,0])\n",
    "#     _pred = cls.predict(test_bag)\n",
    "#     print n, len((_pred == test['label'][:,0]).nonzero()[0]) / float(test_bag.shape[0])\n",
    "#     print me.confusion_matrix(test['label'][:,0], _pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-good-\n",
      "work great;  the best;  a great;  very good;  i love;  love this;  great phone;  all the;  is great;  you wont;  wa good;  i liked;  would recommend;  to go;  loved it;  so good;  the price;  work fine;  loved the;  better than;  \n",
      "-bad-\n",
      "the worst;  would not;  not good;  piece of;  waste of;  wa terrible;  very disappointed;  doe not;  donot work;  so bad;  the battery;  at all;  not work;  i not;  very bad;  wa not;  just donot;  very disappointing;  the script;  it just; \n"
     ]
    }
   ],
   "source": [
    "bowv = NGramVectorizer(2)\n",
    "train_bag = bowv.fit_transform(train['text'][:,0], normalize=None)\n",
    "test_bag = bowv.transform(test['text'][:,0], normalize=None)\n",
    "\n",
    "cls = LR()\n",
    "cls.fit(train_bag, train['label'][:,0])\n",
    "coef = cls.coef_[0]\n",
    "idx = np.argsort(coef)\n",
    "print '-good-'\n",
    "for i in idx[::-1][:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],\n",
    "print '\\n-bad-'\n",
    "for i in idx[:20]:\n",
    "    print '%s; ' % bowv._lookup_voc[i],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NGramVectorizer(BagOfWordsVectorizer):\n",
    "    def __init__(self, N):\n",
    "        super(NGramVectorizer, self).__init__()\n",
    "        self.N = N\n",
    "        \n",
    "    def _gen_ngram(self, tokens):\n",
    "        r = []\n",
    "        for i in range(0, len(tokens) - self.N + 1):\n",
    "            r.append(\" \".join(tokens[i:i+self.N]))\n",
    "        return r\n",
    "        \n",
    "    def tokenize(self, X):\n",
    "        _ = super(NGramVectorizer, self).tokenize(X)\n",
    "        return map(self._gen_ngram, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "class PCA(object):\n",
    "    def fit_transform(self, X, q):\n",
    "        self.u = np.mean(X, axis=0)\n",
    "        self._X = X - self.u\n",
    "        self.V = svd(self._X)[-1]\n",
    "        return self._X.dot(self.V[:q, :].T)\n",
    "    \n",
    "    def transform(self, X, q):\n",
    "        return (X - self.u).dot(self.V[:q, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- KMeans --\n",
      "cluster0 299 285\n",
      "cluster1 901 915\n",
      "--\n",
      "cluster0 177 205\n",
      "cluster1 1023 995\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "print '-- KMeans --'\n",
    "for q in [200, 500]:\n",
    "    bowv = BagOfWordsVectorizer()\n",
    "    pca = PCA()\n",
    "    train_bag = pca.fit_transform(bowv.fit_transform(train['text'][:,0], normalize='l2'), q)\n",
    "    test_bag = pca.transform(bowv.transform(test['text'][:,0], normalize='l2'), q)\n",
    "    \n",
    "    kmeans = KMeans()\n",
    "    kmeans.fit(train_bag, 2)\n",
    "    train_center = kmeans.get_center()\n",
    "    train_status = kmeans.get_status()\n",
    "    cluster0 = train['label'][train_status == 0, 0]\n",
    "    cluster1 = train['label'][train_status == 1, 0]\n",
    "    print \"cluster0\", np.sum(cluster0), \n",
    "    print cluster0.shape[0] - np.sum(cluster0)\n",
    "    print \"cluster1\", np.sum(cluster1), \n",
    "    print cluster1.shape[0] - np.sum(cluster1)\n",
    "    print '--'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Bag-of-words --\n",
      "-- q = 200 -- \n",
      "None 0.768333333333\n",
      "[[232  68]\n",
      " [ 71 229]]\n",
      "l1 0.736666666667\n",
      "[[223  77]\n",
      " [ 81 219]]\n",
      "l2 0.755\n",
      "[[225  75]\n",
      " [ 72 228]]\n",
      "-- q = 500 -- \n",
      "None 0.798333333333\n",
      "[[241  59]\n",
      " [ 62 238]]\n",
      "l1 0.746666666667\n",
      "[[224  76]\n",
      " [ 76 224]]\n",
      "l2 0.775\n",
      "[[231  69]\n",
      " [ 66 234]]\n"
     ]
    }
   ],
   "source": [
    "print '-- Bag-of-words --'\n",
    "for q in [200, 500]:\n",
    "    print '-- q = %d -- ' % q\n",
    "    for n in [None, 'l1', 'l2']:\n",
    "        bowv = BagOfWordsVectorizer()\n",
    "        pca = PCA()\n",
    "        train_bag = pca.fit_transform(bowv.fit_transform(train['text'][:,0], normalize=n), q)\n",
    "        test_bag = pca.transform(bowv.transform(test['text'][:,0], normalize=n), q)\n",
    "\n",
    "        cls = LR()\n",
    "        cls.fit(train_bag, train['label'][:,0])\n",
    "        _pred = cls.predict(test_bag)\n",
    "        print n, len((_pred == test['label'][:,0]).nonzero()[0]) / float(test_bag.shape[0])\n",
    "        print me.confusion_matrix(test['label'][:,0], _pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive namespace is empty.\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
